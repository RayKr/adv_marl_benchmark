用torch.distributions.Distribution来作为动作的选取器，搭配Agent类的perform、collect来使用
Agent中，forward代表有梯度的前向传播，返回的是Distribution类
perform代表的是测试阶段的表现，通过deterministic来决定测试时是否要用随机策略
collect代表的是训练阶段收集数据的表现

关于original_obs、bad_mask：
当游戏超出episode limit时，下一步step会重新开始游戏，但是原本下一时刻的obs会存在info的original_obs里面
original_obs这个东西虽然设计出来了，但是onpolicy算法没用到，所以我觉得offpolicy算法也没什么用的必要，直接删掉original_obs

关于内置的RNNLayer：
可以直接把时序序列传入RNN，支持自动的BPTT
RNN的输入x是(T*N, V)，而hidden_state和masks是(N, V)
当发现他们第0维不同时，RNN会将输入x变换为(T,N,V)的形式，实现BPTT
最后返回的输出的形式依然是(T*N,V)

MADDPG所有interval的单位都改成了timesteps，但是buffer size是episodic，这些参数的意义和HARL不同，不能直接复制！
不share param时，obs和action的shape也得相同，暂时没想支持shape不同的环境，很麻烦

perturbation的目标攻击和minq攻击还没有实现

mode函数只有pytorch版本在1.12及以上时才有

杀掉服务器上所有的星际进程：
ps -A | grep Main_Thread | awk '{print $1}' | xargs kill -9

如何实现一个新的自定义环境：
1. 修改环境本身，或给原有的环境套一个wrapper，使得环境符合amb的接口
2. 修改amb/utils/env_utils.py，在make_train_env, make_eval_env, make_render_env函数中添加自定义环境的信息
3. 修改amb/utils/config_utils.py，在get_task_name中添加自定义环境任务名信息

自定义环境接口介绍：
环境的配置参数全部写在amb/configs/envs_cfgs/{环境名}.yaml中，这些参数会被以字典形式读入，并借由make_xxx_env函数的env_args传入给环境
你的环境类需要满足amb/envs/env_example.py中的所有接口，包括输入输出的类型限制和要求

需要给Dual环境写一个专门的logger