用torch.distributions.Distribution来作为动作的选取器，搭配Agent类的perform、collect来使用
Agent中，forward代表有梯度的前向传播，返回的是Distribution类
perform代表的是测试阶段的表现，通过deterministic来决定测试时是否要用随机策略
collect代表的是训练阶段收集数据的表现

关于original_obs、bad_mask：
当游戏超出episode limit时，下一步step会重新开始游戏，但是原本下一时刻的obs会存在info的original_obs里面
original_obs这个东西虽然设计出来了，但是onpolicy算法没用到，所以我觉得offpolicy算法也没什么用的必要，直接删掉original_obs

关于内置的RNNLayer：
可以直接把时序序列传入RNN，支持自动的BPTT
RNN的输入x是(T*N, V)，而hidden_state和masks是(N, V)
当发现他们第0维不同时，RNN会将输入x变换为(T,N,V)的形式，实现BPTT
最后返回的输出的形式依然是(T*N,V)

MADDPG所有interval的单位都改成了timesteps，但是buffer size是episodic，这些参数的意义和HARL不同，不能直接复制！
关于offpolicy nstep reward再想想办法
不share param时，obs的shape也得相同，暂时没想支持obs shape不同的环境

下一步打算实现runners中的traitor、perturbation等
perturbation支持最小化最大概率的扰动、minq扰动、读取特定target的扰动
traitor支持单方面对抗策略的训练